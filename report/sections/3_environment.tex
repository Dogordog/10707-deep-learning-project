% !TEX root=../report.tex

\section{Multi-Agent Environment}

We use the multi-agent extension of the OpenAI Gym framework~\cite{gym.openai16} to setup our predator prey environment. We employ this framework as it is quickly becoming the standard in terms of environments to benchmark reinforcement learning algorithms in. In particular, the framework defines the state and action space for every agent based on the environment, which allows researchers to focus on making agents act intelligently. The framework is turn based, i.e., each agent performs an action (including no action) at every time step. The environment in turn provides each agent with a reward as well as an observation that describes the environment's state.

\subsection{The Predator-Prey Environment}

In the predator prey environment, slower (red) agents chase faster (green) adversaries. Multiple agents can exist on either team and the goal of each agent is to maximize its own reward in this mixed cooperative and competitive setting. The environment returns a list of states, one per agent, once each agent performs an action. An agent's state consists of the (1) agent's position and velocity, (2) the relative position of any landmarks, (3) the relative position of all other agents. The environment also returns a reward per agent. Agents receives -50 reward if they leaves the arena. If they do, the environment is reset, i.e., the next episode begins in a random configuration. Red agents receive a +100 reward for any green agent they intercept. Green agents, on the other hand, receive a -100 reward if they are intercepted by a red agent. To help training converge faster, we also enabled an l2 penalty. Red agents were given a negative reward proportional to their distance from green agents to prevent them from drifting aimlessly during the early stages of training. Green agents were given a positive reward proportional to their distance from red agents to similarly help them train faster.

The action space consists of a list of actions, one per agent. Each agent's action is described by 5 numbers, which represent whether an agent should stay put or move up/down/left/right (this representation seems to contain redundancies, but it is specified by the environment and should not be modified by the algorithm designer). Each number is clamped between 0 and 1. 

For this project, we trained our reinforcement learning models on 3 scenarios in the predator prey setting - 1) 1 red agent vs 1 green agent, 2) 2 green agents vs 1 red agent and 3) 1 green agent vs 2 red agents. We did not use any landmarks, as none of the algorithms we employ (DQN, DDPG and MADDPG) explicitly model the landmarks in their optimization, i.e., there is no constraint/penalty that prevents agents from attempting to "move through" the landmarks. 

