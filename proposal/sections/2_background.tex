% !TEX root=../proposal.tex

\section{Background}
\label{sec:background}

\subsection{Multi-Agent Reinforcement Learning Problem Setting}

In traditional single-agent reinforcement learning, the problem is set up as a
\emph{Markov Decision Process} (MDP). In a MDP problem setting, the agent can observe
a finite set of states $S$, take a finite set of actions $A$, has a state
transition function $T(s'|s,a) = P(S_{t+1}=s'|S_t=s, A_t=a)$, has a reward
function $r(s,a) = E[R_{t+1}|S_t=s, A_t=a]$, and a discount factor $\gamma$.
The state is fully observable to the agent, and the agent can decide a policy
$\pi(a|s) = P(A_t=a | S_t=s)$ for its next action based on the current state
$s$.

There are two ways for the agent to decide a good policy that maximizes the
return (i.e., discounted reward) $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$.
First, the agent can estimate the \emph{state-value function} of possible policies
$v_\pi(s) = E_\pi[G_t|S_t=s]$. Secondly, the agent can alternatively
estimate the \emph{action-value function} of possible policies
$q_\pi(s,a) = E_\pi[G_t|S_t=s, A_t=a]$. These functions essentially estimates
the expected return of a certain policy. To choose the best policy, the agent
estimates either of these value functions for a range of possible policies and
pick the one that has the highest value (i.e., expected return).

In multi-agent reinforcement learning, there are multiple agents, which leads
to several differences: (1)~each agent observes potentially different subsets
of states $O_1,\ldots,O_N$, which depend on the overall state, (2)~each agent
takes different actions $A_1,\ldots,A_N$, (3)~the state transition function
depends on the actions of all agents, (4)~each agent may have different goals
and thus have its own reward functions.

\subsection{Q-Learning and Deep Q-Learning}



\subsection{Evolutionary Algorithms}

Evolutionary algorithms are randomized algorithm which search through a
parameter space using operations such as random mutation and mixing of
parameter combinations~\cite{man1996genetic}. These sort of algorithms are
embarrassingly parallel and can be applied to optimize reinforcement learning
parameters. This sort of approach, which treats the reinforcement learning
problem as a black box, has been used to scale a Mujoco Humanoid problem to
1440 cores~\cite{salimans2017evolution}.

The idea of using these algorithms for RL is that the simplicity of brute
force trumps the complexity of other optimizers. Evolutionary algorithms have
low communication overhead, no backpropagation calculations, high parallelism
and an optimization algorithm free of stability issues, and they do this while
being competitive in training progress~\cite{salimans2017evolution}. Some of
the earliest RNNs were trained with random search, where they surpassed
backpropagation methods, so this trend has been seen before in the ML
community~\cite{hochreiter1997long}.

