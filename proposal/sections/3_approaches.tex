% !TEX root=../proposal.tex

\section{Approach}
\label{sec:approach}

\subsection{Deep Deterministic Policy Gradient (DDPG)}
DDPG is an actor critic method that adapts the underyling success of Deep Q-learning 
to the continuous action domain. DDPG itself is based on the deterministic
policy gradient algorithm (DPG)~\cite{silver2014deterministic} which rewrites the gradient
of the objective $J(\theta)$(Section 2.3) as:
\begin{equation}
\nabla_{\theta}J(\theta) = E_{s \sim D} [\nabla_{\theta}\mu_{\theta}(a|s)\nalba_{a} Q^{\mu}(s,a)|_{a = \mu_{\theta}(s)}]
\end{equation}
where $\mu_{\theta}: S -> A$ are deterministic policies. DDPG approximates
both the policy $\mu$ and the critic $Q^{\mu}$ with deep neural networks, sampling
trajectories from a replay buffer of experiences and using a target network as 
in DQN. Note that in the multi agent setting, each agent is trained individually
with DDPG, there is no combined optimization of the agents. However, the MADDPG 
algorithm, explained in the next section, naturally extends DDPG to the multi-agent
setting during training, resulting in much richer behavior between agents. 

\subsection{Multi-Agent Actor Critic for Mixed Cooperative-Competitive Environments (MADDPG)}
MADDPG is a recently developed general-purpose multi-agent learning algorithm
that is a simple extension of actor-critic policy gradient methods where the
critic is augmented with extra information about the policies of other agents
while the actor only has access to local information (i.e., its own
observations). In this framework of centralized training with decentralized
execution, agents don’t need to access the central critic at test time; they
learn approximate models of other agents and effectively use them in their own
policy learning procedure. Furthermore, since the centralized critic is
learned independently for each agent, this approach can in principle be used
to model arbitrary reward structures between agents, including adversarial
cases where the rewards are opposing.

The primary task we plan to apply MADDPG to is learning adaptive strategies
for coordinating/controlling traffic signals with the aim of increasing
traffic flow through multiple intersections. Unlike previous attempts that
employ Q learning to model adaptive traffic policies~\cite{araghi2015traffic},
we hypothesize that employing a centralized critic to supply agents (traffic
signals) with information about their peers’ observations (the amount of
traffic at intersections) and potential actions will lead to more stable
training and overcome the shortcomings of techniques not naturally suited to
multi-agent environments.

Furthermore, we will explore communication between multiple agents in the
presence of noisy channels. In the traffic signal coordination task, one can
easily imagine situations where communication channels between traffic signals
(assuming such channels existed in the first place) get corrupted or break. In
this scenario, agents will have to communicate reliably with one another
despite their messages being dropped or corrupted randomly. Prior work has
looked at using linear block error correcting codes designed by experts that
are then decoded by deep neural networks~\cite{nachmani2016learning,
nachmani2017rnn}. We plan to build on this work and incorporate it into the
MADDPG framework to increase the error tolerance.

\subsection{An Evolutionary Approach to Multi Agent Systems}
Finally, we will investigate certain optimizations in training such as
evolutionary strategies, which have been shown to be competitive in training
to traditional methods while allowing increased
parallelism~\cite{salimans2017evolution}.
Increased parallelism and scalability are important for allowing larger models to be trained~\cite{nair2015massively}.
We will investigate both algorithmic and computer systems optimizations which may make this a viable training technique.
Namely, we will investigate how the quantity and quality of updates affect training performance.
If these methods prove successful, they can be incorporated with other training methods to increase learning scalability.

We will use evolutionary techniques to brute force the problems.
The main idea is to see if the quantity of updates will prevail over the quality over updates.
We are equipped to solve this problem as we have access to a cluster with over $300$ nodes with $4$ cores per node.
This yields over $1200$ cores total, which is on the same order of magnitude as the cluster sizes used for state of the art research~\cite{salimans2017evolution}.
There are many metrics worth considering here, but the main ones will measure learning progress per unit time.
This is because for every update a traditional algorithm will due, the evolutionary strategy can do thousands.

One avenue worth exploring is whether this sort of problem can be optimized and deployed effectively over GPUs.
Evolutionary algorithms are embarassingly parallel, but it's not necessarily true that they will run well on GPUs.
Furter, it’s may be worth investigating how parameter sharing impacts an evolutionary search.
Sharing parameters decreases the complexity of the learning task, however it increases the required communication required in the system.
Allowing parameters to grow stale may help counteract some of these negative effects~\cite{cui2014exploiting}.
