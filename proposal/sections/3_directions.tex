% !TEX root=../proposal.tex

\section{Proposed Approach}
\label{sec:direction}

We propose to explore new ways based on deep neural networks for multiple
agents to cooperatively communicate with each other. Such cooperative
communication scenario includes robust communication through a noisy channel.
In this scenario, two agents have to reliably communicate with each other
through messages that can be dropped or corrupted randomly. Many prior work
have looked at using linear block error correcting codes designed by experts
and use deep neural network to decode the message~\cite{nachmani2016learning,
nachmani2017rnn}. We plan to explore new neural network architectures to
decode these existing codes more efficiently as well as new encoding methods
using deep neural networks to further increase the error tolerance. Another
example of cooperative communication is covert communication where two agents
securely communicate over a public channel without other adversarial agents
finding out the message. Recent works have looked at using GANs to generate
new encryption and decryption functions~\cite{abadi2016learning}.

From the paper/blog:

In this work, we propose a general-purpose multi-agent learning algorithm
that: (1) leads to learned policies that only use local information (i.e.
their own observations) at execution time and (2) does not assume a
differentiable model of the environment dynamics or any particular structure
on the communication method between agents.

Similarly to [8], we accomplish our goal by adopting the framework of
centralized training with decentralized execution. Thus, we allow the policies
to use extra information to ease training, so long as this information is not
used at test time. It is unnatural to do this with Q-learning, as the Q
function generally cannot contain different information at training and test
time. Thus, we propose a simple extension of actor-critic policy gradient
methods where the critic is augmented with extra information about the
policies of other agents while the actor only has access to local information.
After training is completed, only the local actors are used at execution
phase, acting in a decentralized manner and equally applicable in cooperative
and competitive settings.

Since the centralized critic function explicitly uses the decision-making
policies of other agents, we additionally show that agents can learn
approximate models of other agents online and effectively use them in their
own policy learning procedure.

Include some math from Section 4 of the paper. 
